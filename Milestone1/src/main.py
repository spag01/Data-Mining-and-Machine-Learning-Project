# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x_zkla0RB-gLaKz06SQrNRGfibgB6n9g
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import math

#Getting data
#Enter your path for files
case_train = pd.read_csv("cases_2021_train.csv")
case_test = pd.read_csv("cases_2021_test.csv")
location_data = pd.read_csv("location_2021.csv")

hospitalized_set = {'Discharged', 'Discharged from hospital', 'Hospitalized', 'critical condition', 'discharge', 'discharged'}
non_hospitalized_set = {'Alive', 'Receiving Treatment', 'Stable', 'Under treatment','recovering at home 03.03.2020', 'released from quarantine','stable', 'stable condition'}
deceased_set = {'Dead', 'Death', 'Deceased', 'Died', 'death', 'died'}
recovered_set = {'Recovered', 'recovered'}

#1.1 Putting in four distinct groups
case_train.loc[(case_train['outcome'] == "recovered") | (case_train['outcome'] == "Recovered"),'outcome_group']="recovered"
case_train.loc[(case_train['outcome'] == "Alive") | (case_train['outcome'] == "Receiving Treatment")| (case_train['outcome'] == "Stable")| (case_train['outcome'] == "Under treatment")
               |(case_train['outcome'] == "recovering at home 03.03.2020")| (case_train['outcome'] == "released from quarantine")| (case_train['outcome'] == "stable")
               |(case_train['outcome'] == "stable condition"),'outcome_group']="nonhospitalized"
case_train.loc[(case_train['outcome'] == "Dead") | (case_train['outcome'] == "Death")| (case_train['outcome'] == "Deceased")| (case_train['outcome'] == "Died")| (case_train['outcome'] == "death")| (case_train['outcome'] == "died"),'outcome_group']="deceased"
case_train.loc[(case_train['outcome'] == "Discharged") | (case_train['outcome'] == "Discharged from hospital")| (case_train['outcome'] == "discharged")| (case_train['outcome'] == "Hospitalized")| (case_train['outcome'] == "critical condition")| (case_train['outcome'] == "discharge"),'outcome_group']="hospitalized"

#1.1 Report the number of cases for each outcome_group label.
case_train.groupby('outcome_group').size()

#1.1 remove the outcome column from cases_2021_train.csv.

case_train = case_train.drop('outcome', axis=1)

"""# 1.2 Outcome labels

Q. What type of data mining task is the prediction of the outcome_group labels in the
cases_2021_train.csv and cases_2021_test.csv datasets?

A. Clustering - identify data objects that are similar to one another

#1.4 Discuss how you reduced the different formats and justify your approach. 


Possible formats:
1.   20-25 (In a range) 
2.  80- (Missing the end range)
3.  0.1 (Decimals)

For ages in a range (#1), the ceiling of median of the two numbers are used. 
*   e.g. 20-24 becomes 22
*   20-25 becomes 23 because it is rounded up from 22.5

For ages that are missing the end range (#2), the starting range of the two numbers are used.
*   e.g. 80- becomes 80

For ages that are decimals (#2), the rounded up number is used.
*   e.g. 0.1 becomes 1
"""

#1.4 Dropping missing age values

case_train.dropna(subset = ["age"], inplace=True)
case_test.dropna(subset = ["age"], inplace=True)

#1.4 Reducing different formats to a standard integer format

from ctypes import resize
def map_fn(age: str):
  if '-' in age:
    res = age.split('-')
    if res[0] and res[1]:
      n1, n2 = res[0], res[1]
      if n1 and n2:
        ret = math.ceil((int(n1)+int(n2)) // 2)
        return ret
    else:
        return res[0]
  else:
    return age

case_train['age'] = case_train['age'].apply(map_fn)
case_train['age'] = case_train['age'].astype(float)
case_train['age'] = case_train['age'].round(decimals = 0)
case_train['age'] = case_train['age'].astype(int)

#1.4 Reducing different formats to a standard integer format
case_test['age'] = case_test['age'].apply(map_fn)
case_test['age'] = case_test['age'].astype(float)
case_test['age'] = case_test['age'].round(decimals = 0)
case_test['age'] = case_test['age'].astype(int)

#1.4 Data cleaning and imputing missing values

def impute_case(df):
  df['sex'].fillna("Missing", inplace=True)
  df['province'].fillna("Missing", inplace=True)
  df['country'].fillna("Missing", inplace=True)
  df['latitude'].fillna("Missing", inplace=True)
  df['longitude'].fillna("Missing", inplace=True)
  df['date_confirmation'].fillna("Missing", inplace=True)
  df['additional_information'].fillna("Missing", inplace=True)
  df['source'].fillna("Missing", inplace=True)
  df['chronic_disease_binary'].fillna("Missing", inplace=True)

  return df

case_test = impute_case(case_test)
case_train = impute_case(case_train)
# location_data.fillna(0)

case_test

case_train

location_data

"""#1.5 
Outliers are often either extremely low or extremely high values.

During our invesigation, we noticed several outliers in each of the datasets.

We identified the outliers via the conventional sorting and visualization method. Upon sorting the data for both case_test.csv and case_train.csv, we identified that a small subset of age values with a wide range, e.g. 22-80. For age values with a small range, it may be possible to take the average of the two the range as the estimated age. However, for wide ranges that have a difference of >= 20, the calculated average will certainly not be representative of the actual age. 

Once an outlier has been identified, a decision must be made as whether to retain or remove them from the dataset. In the event that the outliers are clearly erorrs or bad data, they should be removed from the dataset. Otherwise, outliers should be accepted as much as possible.
 
In this scenario, it is not obvious that the outliers detected are errors. Furthermore, the outliers do not present a significant impact on a large dataset - the central tendency and variablility of the data will not be affected by a small number of extreme values. For this reason, we have adopted to retain these outliers.

#1.6 Joining the cases and location dataset

The case and location datasets were joined on the basis of 'province, country'. The motivation for joining the datasets was to provide additional meaningful information to the case datasets - 'Confirmed, Deaths, Recovered, Active, Incident_Rate, Case_Fatality_Ratio'.

The addition of the new attributes will allow readers to gain a better sense of the geographical statistic as a whole.

And for joining, left join is used because on the left we kept the main dataframes (train and test) and joined with location dataset which is on right.

The number of rows in joined train datset are 23,054.

The number of rows in joined test dataset are 11,373.
"""

#1.6 Fixing country part of location data
location_data.loc[location_data['Country_Region'] == "US", 'Country_Region'] = "United States"
location_data.loc[location_data['Country_Region'] == "Korea, South", 'Country_Region'] = "South Korea"
location_data

location_data.to_csv("location_2021_processed.csv")

#1.6 Cleaning location data for joining
location_data = location_data.drop(['Lat', 'Long_','Last_Update','Combined_Key'], axis=1)
location_data

#1.6 Cleaning location data for joining 
location_data = location_data.groupby(['Province_State','Country_Region']).sum()
location_data

#1.6 Joining location with train data
final_case_train = pd.merge(case_train, location_data, how='left', left_on=['province','country'],right_on=['Province_State','Country_Region'])
final_case_train

#1.6 Joining location with test data 
final_case_test = pd.merge(case_test, location_data, how='left', left_on=['province','country'],right_on=['Province_State','Country_Region'])
final_case_test

final_case_test.to_csv("cases_2021_test_processed.csv")

final_case_train.to_csv("cases_2021_train_processed.csv")

"""#1.7 Feature selection (15 marks)

In this question we performed, feature selection technique where we selected the relevant attributes from joined test and train datasets and wrote them into csv files. 

From 1.6, we selected age, sex, province, country, date_confirmation, additional response, source, chronic disease, outcome_group, Confirmed, Deaths, Recovered, Active, Incident Rate, and Case Fatality Rato as are final relevant attributes for 1.7.

And we dropped longitude and latitude attributes from 1.6.
"""

#1.7 Selecting relevant features and dropping irrelevant features
final_case_test_1 = final_case_test.drop(['latitude', 'longitude'], axis=1)
final_case_test_1

#1.7 Selecting relevant features and dropping irrelevant features
final_case_train_1 = final_case_train.drop(['latitude', 'longitude'], axis=1)
final_case_train_1

final_case_test_1.to_csv("cases_2021_test_processed_features.csv")

final_case_train_1.to_csv("cases_2021_train_processed_features.csv")